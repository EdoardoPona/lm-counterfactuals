{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a0d41716-2925-406a-b74b-88dcadbf560e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "from transformers.generation import LogitsProcessor,LogitsProcessorList\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.stats import gumbel_l\n",
    "import copy\n",
    "import torch\n",
    "import tqdm\n",
    "import scipy\n",
    "from scipy.stats import gumbel_l, gumbel_r\n",
    "import transformers\n",
    "from evaluate import load\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# import nn\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e0df248e-d07b-4eaf-b502-ba09f4f22115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_name = \"HuggingFaceTB/SmolLM-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,trust_remote_code=True,torch_dtype=torch.float16,\n",
    "                                             device_map='auto').eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4317d712-da56-4143-b56e-92d3a22fddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "533c97c7-b1d8-43aa-bdd1-76ab3a81b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_logits(model, tokenizer, input_text, max_new_tokens=50, stop_token=None, noise=None):\n",
    "    \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    #print([tokenizer.decode(tok) for tok in input_ids])\n",
    "    # Initialize variables\n",
    "    generated_ids = input_ids\n",
    "    past_key_values = None  # for caching the past key/values\n",
    "    \n",
    "    for i in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(generated_ids[:, :], past_key_values=past_key_values, use_cache=False)\n",
    "        \n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        #print(\"argmaxed token:\", tokenizer.decode(logits.argmax()))\n",
    "        if noise is not None:\n",
    "            logits += noise[i]\n",
    "        #print(\"argmaxed token after noise:\", tokenizer.decode(logits.argmax()))\n",
    "        past_key_values = outputs.past_key_values\n",
    "        next_token_id = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "        next_token_id_scalar = next_token_id.item()\n",
    "        \n",
    "        generated_token = tokenizer.decode([next_token_id_scalar], skip_special_tokens=True)\n",
    "        \n",
    "        if stop_token and generated_token == stop_token:\n",
    "            break\n",
    "        if next_token_id_scalar == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    # Decode the full generated sequence\n",
    "    final_sentence = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return final_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7d1a5601-d4ed-414a-afcb-89f39b339c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>Hola a todos!']\n",
      "argmaxed token: \n",
      "\n",
      "argmaxed token: I\n",
      "argmaxed token: ’\n",
      "argmaxed token: m\n",
      "argmaxed token:  a\n",
      "argmaxed token:  Spanish\n",
      "argmaxed token:  teacher\n",
      "argmaxed token:  and\n",
      "argmaxed token:  I\n",
      "argmaxed token: ’\n",
      "argmaxed token: m\n",
      "argmaxed token:  going\n",
      "argmaxed token:  to\n",
      "argmaxed token:  teach\n",
      "argmaxed token:  you\n",
      "argmaxed token:  how\n",
      "argmaxed token:  to\n",
      "argmaxed token:  say\n",
      "argmaxed token:  “\n",
      "argmaxed token: I\n",
      "argmaxed token: ’\n",
      "argmaxed token: m\n",
      "argmaxed token:  sorry\n",
      "argmaxed token: ”\n",
      "argmaxed token:  in\n",
      "argmaxed token:  Spanish\n",
      "argmaxed token: .\n",
      "argmaxed token: \n",
      "\n",
      "argmaxed token: I\n",
      "argmaxed token: ’\n",
      "argmaxed token: m\n",
      "argmaxed token:  sorry\n",
      "argmaxed token:  in\n",
      "argmaxed token:  Spanish\n",
      "argmaxed token: \n",
      "\n",
      "argmaxed token: The\n",
      "argmaxed token:  Spanish\n",
      "argmaxed token:  word\n",
      "argmaxed token:  for\n",
      "argmaxed token:  “\n",
      "argmaxed token: I\n",
      "argmaxed token: ’\n",
      "argmaxed token: m\n",
      "argmaxed token:  sorry\n",
      "argmaxed token: ”\n",
      "argmaxed token:  is\n",
      "argmaxed token:  “\n",
      "argmaxed token: Lo\n",
      "argmaxed token:  s\n",
      "argmaxed token: ient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hola a todos!\\nI’m a Spanish teacher and I’m going to teach you how to say “I’m sorry” in Spanish.\\nI’m sorry in Spanish\\nThe Spanish word for “I’m sorry” is “Lo sient'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = tokenizer.bos_token + \"Hola a todos!\"\n",
    "generate_with_logits(model, tokenizer, prompt, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "517682ef-f4e4-49ca-8e20-c003e4a4b328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '<|endoftext|>Hola a todos!\\nI’m a Spanish teacher and I’m going to teach you how to say “I’m sorry” in Spanish.\\nI’m sorry in Spanish\\nThe Spanish word for “I’m'}]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "# generate text\n",
    "generator(prompt, max_length=50, num_return_sequences=1, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7fde2c27-2566-4f77-a53f-4e5033d7828a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>Hola a todos!\\nI’m a Spanish teacher and I’m going to'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(inputs)\n",
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "ffff98e2-b072-475a-856d-f69555822325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sampling import counterfactual_generation, counterfactual_generation_batched, sample_from_truncated_gumbel_vectorized, counterfactual_generation_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "efdf6a7f-7652-4392-96c7-2e7681fb6ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'It', ' was', ' said', ' wa', ' was', ' ', '2']\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.get_vocab())\n",
    "noise = counterfactual_generation_vectorized(model, tokenizer, \"It was said wa was 2\", vocab_size,prompt=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "94ef286d-4db1-49af-93b6-8fa655f5a964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was said wa was 2'"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_logits(model, tokenizer, tokenizer.bos_token, max_new_tokens=50, noise=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "2064f2f3-8b19-49c2-accb-2e436d54ca44",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2TokenizerFast' object has no attribute 'w2i'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[234], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw2i\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2TokenizerFast' object has no attribute 'w2i'"
     ]
    }
   ],
   "source": [
    "tokenizer.w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "9da7d3de-42cf-40ac-8db0-e3f9b852e729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 49152)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noised_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "199b4a94-cb16-4cfc-a574-68d09a811468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1589 It\n",
      "436  was\n",
      "1137  said\n",
      "436  was\n",
      "0 <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for row in noised_logits[0]:\n",
    "    print(row.argmax(), tokenizer.decode(row.argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "38a15ba4-d57c-4c06-bfae-0e71955e4b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49152/49152 [00:09<00:00, 5303.23it/s]\n",
      "100%|██████████| 49152/49152 [00:09<00:00, 5336.58it/s]\n",
      "100%|██████████| 49152/49152 [00:09<00:00, 5330.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it was claimed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49152/49152 [00:08<00:00, 5486.90it/s]\n",
      "100%|██████████| 49152/49152 [00:09<00:00, 5382.39it/s]\n",
      "100%|██████████| 49152/49152 [00:09<00:00, 5364.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it was claimed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 22227/49152 [00:04<00:04, 5424.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[216], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mcounterfactual_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mit was claimed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/causality/sampling.py:184\u001b[0m, in \u001b[0;36mcounterfactual_generation\u001b[0;34m(model, tokenizer, sentence, vocab_size, prompt)\u001b[0m\n\u001b[1;32m    182\u001b[0m logit_j \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m][i][j]\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m#np.random.seed(0)\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43msample_from_truncated_gumbel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcdf_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogit_w\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogit_j\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgumbel_l\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m logit_diffs\u001b[38;5;241m.\u001b[39mappend(logit_w \u001b[38;5;241m-\u001b[39m logit_j)\n\u001b[1;32m    186\u001b[0m gumbel_noise\u001b[38;5;241m.\u001b[39mappend(sample)\n",
      "File \u001b[0;32m~/causality/sampling.py:247\u001b[0m, in \u001b[0;36msample_from_truncated_gumbel\u001b[0;34m(cdf_a, b, gumbel, seed)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#first_word = sentence.split(\" \")[0]\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m#prompt = tokenizer.bos_token if prompt is None else prompt\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample_gumbel(model, tokenizer, processor, tokenizer\u001b[38;5;241m.\u001b[39mbos_token)\u001b[38;5;66;03m#, all_logit_diffs, all_gumbel_noise, processor\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_from_truncated_gumbel\u001b[39m(cdf_a,b,gumbel,seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    248\u001b[0m     cdf_b \u001b[38;5;241m=\u001b[39m gumbel\u001b[38;5;241m.\u001b[39mcdf(b)\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m#np.random.seed(0) if seed is None else np.random.seed(seed)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(counterfactual_generation(model, tokenizer,   \"it was claimed\", vocab_size,prompt=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e9465b9c-eb1b-440e-868a-a29eb93c2204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_from_truncated_gumbel(cdf_a,b,gumbel,seed=None):\n",
    "    cdf_b = gumbel.cdf(b)\n",
    "    #np.random.seed(0) if seed is None else np.random.seed(seed)\n",
    "    u = np.random.uniform(0, 1)\n",
    "    if cdf_b < cdf_a:\n",
    "        cdf_a, cdf_b = cdf_b, cdf_a\n",
    "    return gumbel.ppf(cdf_a + u * (cdf_b - cdf_a))\n",
    "\n",
    "sample_from_truncated_gumbel(-10,0,gumbel_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c495ffd-4093-4b18-8841-50428675dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(counterfactual_generation(model, tokenizer,   \"it was claimed that the king has\", vocab_size,prompt=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ba830-2fd9-4bb7-95b2-c97f87677715",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(counterfactual_generation(model, tokenizer,   \"asdsad rhtut dzc a rt\", vocab_size,prompt=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61f2590c-7eef-49b2-b0c7-513d7ca5da28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.61691634,  0.24225987, -9.07597244])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gumbel_l.ppf(np.random.rand(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9abdb37a-7d02-4d7f-bb4e-19041cce53ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.12457641e-218, 7.12457641e-218, 7.12457641e-218, 7.12457641e-218,\n",
       "       7.12457641e-218])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gumbel_l.cdf(np.ones(5) * (-500.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "32c357eb-7e84-4133-a866-b443a2914a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.99435995 -0.38364933 -9.48239606]\n",
      "[-0.99435995 -0.38364933 -9.48239606]\n"
     ]
    }
   ],
   "source": [
    "def sample_from_truncated_gumbel_vectorized(cdf_a, b):\n",
    "    cdf_a = np.zeros_like(b)\n",
    "    #return np.array([sample_from_truncated_gumbel(cdf_a[i], b[i], gumbel_l) for i in range(len(b))])\n",
    "\n",
    "def sample_from_truncated_gumbel(cdf_a,b,gumbel,u,seed=None):\n",
    "    cdf_b = gumbel.cdf(b)\n",
    "    np.random.seed(0) if seed is None else np.random.seed(seed)\n",
    "    return gumbel.ppf(cdf_a + u * (cdf_b - cdf_a))\n",
    "\n",
    "u = np.random.rand(3)\n",
    "cdf_a = np.zeros(3)\n",
    "b = np.random.rand(3)\n",
    "cdf_b = gumbel_l.cdf(b)\n",
    "print(np.array([sample_from_truncated_gumbel(cdf_a[i], b[i], gumbel_l, u[i]) for i in range(len(cdf_b))]))\n",
    "print(gumbel_l.ppf(cdf_a + u * (cdf_b - cdf_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d1fa0-695e-4635-9661-d82b550dd6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
