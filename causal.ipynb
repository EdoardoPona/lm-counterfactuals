{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation import LogitsProcessor,LogitsProcessorList\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.stats import gumbel_l\n",
    "from arsenal.maths.rvs import TruncatedDistribution\n",
    "import copy\n",
    "import torch\n",
    "import tqdm\n",
    "import scipy\n",
    "from scipy.stats import gumbel_l, gumbel_r\n",
    "from arsenal.maths.rvs import TruncatedDistribution\n",
    "import transformers\n",
    "from evaluate import load\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "import ot\n",
    "import sk2torch\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# import nn\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"openai-community/gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,trust_remote_code=True,torch_dtype=torch.float16,\n",
    "                                             device_map='auto').eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode(model, tokenizer, text, batch_size,layer=-1, pooling=\"last\"):\n",
    "  encodings = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "   for i in tqdm.tqdm(range(0, len(text), batch_size)):\n",
    "    batch = text[i:i+batch_size]\n",
    "    padded_tokens = tokenizer(batch, padding=True, return_tensors=\"pt\", max_length=128, truncation=True).to(\"cuda\")\n",
    "    outputs = model(**padded_tokens, output_hidden_states=True)\n",
    "    lengths = padded_tokens[\"attention_mask\"].sum(axis=1).detach().cpu().numpy()\n",
    "\n",
    "    hiddens = outputs.hidden_states[layer]\n",
    "    hiddens = hiddens.detach()\n",
    "    for h,l in zip(hiddens, lengths):\n",
    "      if pooling == \"last\":\n",
    "        h = h[l-1]\n",
    "      elif pooling == \"cls\":\n",
    "        h = h[0]\n",
    "      elif pooling == \"mean\":\n",
    "        h = h[:l].mean(axis=0)\n",
    "      encodings.append(h.detach().cpu().numpy())\n",
    "\n",
    "  return np.array(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bios_data/bios_data/train.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    y = np.array([d[\"p\"] for d in data])\n",
    "    z = np.array([1 if d[\"g\"] == \"m\" else 0 for d in data])\n",
    "    texts = [d[\"text\"] for d in data]\n",
    "\n",
    "y_to_keep = [\"professor\", \"physician\", \"attorney\"]\n",
    "idx_to_keep = [i for i in range(len(y)) if y[i] in y_to_keep]\n",
    "y = y[idx_to_keep]\n",
    "z = z[idx_to_keep]\n",
    "texts = [texts[i] for i in idx_to_keep]\n",
    "\n",
    "num_m, num_f = sum(z), len(z) - sum(z)\n",
    "n = 10000#min(num_m, num_f)\n",
    "idx_m = [i for i in range(len(z)) if z[i] == 1]\n",
    "idx_f = [i for i in range(len(z)) if z[i] == 0]\n",
    "idx = idx_m[:n] + idx_f[:n]\n",
    "y = y[idx]\n",
    "z = z[idx]\n",
    "texts = [texts[i] for i in idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-35): 36 x GPT2Block(\n",
       "    (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GPT2Attention(\n",
       "      (c_attn): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [01:31<00:00,  6.85it/s]\n"
     ]
    }
   ],
   "source": [
    "layer = 16\n",
    "encodings = encode(model, tokenizer, texts, 32, layer=layer, pooling=\"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def get_optimal_gaussian_transport_func(source_x, target_x):\n",
    "      cov_source = np.cov(source_x.T).real + 1e-8\n",
    "      cov_target = np.cov(target_x.T).real + 1e-8\n",
    "\n",
    "      # optimal transport\n",
    "\n",
    "      cov_source_sqrt = matrix_squared_root(cov_source)\n",
    "      cov_source_sqrt_inv = matrix_inv_squared_root(cov_source) #scipy.linalg.inv(cov_source_sqrt)\n",
    "\n",
    "      A = cov_source_sqrt_inv @ matrix_squared_root(cov_source_sqrt @ cov_target @ cov_source_sqrt) @ cov_source_sqrt_inv\n",
    "      return A\n",
    "\n",
    "def matrix_squared_root(A):\n",
    "    return scipy.linalg.sqrtm(A)\n",
    "\n",
    "\n",
    "def matrix_inv_squared_root(A):\n",
    "\n",
    "    return np.linalg.inv(matrix_squared_root(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_source = encodings[z==0,:][:]\n",
    "x_train_target = encodings[z==1,:][:]\n",
    "A = get_optimal_gaussian_transport_func(x_train_source, x_train_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, max_iter=10000, tol=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, max_iter=10000, tol=0.0001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(loss='log_loss', max_iter=10000, tol=0.0001)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_source = x_train_source.mean(axis=0)\n",
    "mean_target = x_train_target.mean(axis=0)\n",
    "encodings_transformed = encodings.copy()\n",
    "encodings_transformed[z == 0] = mean_target + (encodings_transformed[z == 0] - mean_source) @ A\n",
    "clf = SGDClassifier(loss=\"log_loss\", max_iter=10000, tol=1e-4)\n",
    "clf.fit(encodings, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterventionModule(nn.Module):\n",
    "    def __init__(self, mean_0, mean_1, A, mlp, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.mean_0 = torch.tensor(mean_0)\n",
    "        self.mean_1 = torch.tensor(mean_1)\n",
    "        self.A = torch.tensor(A)\n",
    "        self.mlp = sk2torch.wrap(mlp)\n",
    "        self.alpha = alpha\n",
    "        # set requires_grad=False to all params of the mlp\n",
    "        for p in self.mlp.parameters():\n",
    "            p.requires_grad = False\n",
    "    \n",
    "    def to_cuda(self, device):\n",
    "      self.A = self.A.to(device)\n",
    "      self.mean_0 = self.mean_0.to(device)\n",
    "      self.mean_1 = self.mean_1.to(device)\n",
    "      self.mlp = self.mlp.to(device)\n",
    "\n",
    "    def to_cpu(self):\n",
    "      self.A = self.A.to(\"cpu\")\n",
    "      self.mean_0 = self.mean_0.to(\"cpu\")\n",
    "      self.mean_1 = self.mean_1.to(\"cpu\")\n",
    "      self.mlp = self.mlp.to(\"cpu\")\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        self.to_cuda(hidden_states.device)\n",
    "        # fi hidden state is half, convert laso the params to half precision\n",
    "        if hidden_states.dtype == torch.float16:\n",
    "          self.A = self.A.half()\n",
    "          self.mean_0 = self.mean_0.half()\n",
    "          self.mean_1 = self.mean_1.half()\n",
    "          self.mlp = self.mlp.half()\n",
    "\n",
    "        \n",
    "        preds = self.mlp(hidden_states)\n",
    "        preds = preds[0]\n",
    "\n",
    "        x = hidden_states.clone().reshape(-1, hidden_states.shape[-1])\n",
    "        \n",
    "        x[preds == 0] = self.alpha*self.mean_1 + (x[preds == 0] - self.alpha*self.mean_0)@self.A\n",
    "        #print(\"Steering {} samples\".format((preds == 0).sum()))\n",
    "        x = x.reshape(hidden_states.shape)\n",
    "        return x\n",
    "\n",
    "def insert_intervention(model, model_name, intervention, layer, after_layer_norm=False, replace_existing=False):\n",
    "    if \"gpt2\" in model_name.lower():\n",
    "        # if the mlp is already a Sequential object, do nothing\n",
    "        if isinstance(model.transformer.h[layer].mlp, torch.nn.Sequential):\n",
    "            return\n",
    "        if not replace_existing:\n",
    "            model.transformer.h[layer].mlp = torch.nn.Sequential(model.transformer.h[layer].mlp, intervention)\n",
    "        else:\n",
    "            \n",
    "            model.transformer.h[layer].mlp = torch.nn.Sequential(model.transformer.h[layer].mlp[:-1], intervention)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only GPT2 is supported\")\n",
    "\n",
    "\n",
    "def remove_intervention(model, model_name, layer):\n",
    "    if \"gpt2\" in model_name.lower():\n",
    "        # if the mlp is not a Sequential object, do nothing\n",
    "        if not isinstance(model.transformer.h[layer].mlp, torch.nn.Sequential):\n",
    "            return\n",
    "        model.transformer.h[layer].mlp = model.transformer.h[layer].mlp[0]\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only GPT2 is supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_module = InterventionModule(mean_source, mean_target, A, clf, alpha=5.0)\n",
    "remove_intervention(model, model_name, layer)\n",
    "insert_intervention(model, model_name, intervention_module, layer, replace_existing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Janet Kohn is a professor at NYU, where he teaches about human rights and international law. \"It may be a little unfair to pick on those who do that work. But the other people in the system that say, \\'He must have thought of this,\\' well that\\'s how we get to pick our own people.\"\\n\\nAnd on the \"deep state\"'}]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "# generate text\n",
    "prompt = \"Janet Kohn is a professor at NYU, where\"\n",
    "generator(prompt, max_length=75, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'She is a'}]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
