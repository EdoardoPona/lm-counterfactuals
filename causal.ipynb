{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers.generation import LogitsProcessor,LogitsProcessorList\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.stats import gumbel_l\n",
    "from arsenal.maths.rvs import TruncatedDistribution\n",
    "import copy\n",
    "import torch\n",
    "import tqdm\n",
    "import scipy\n",
    "from scipy.stats import gumbel_l, gumbel_r\n",
    "from arsenal.maths.rvs import TruncatedDistribution\n",
    "import transformers\n",
    "from evaluate import load\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "import ot\n",
    "import sk2torch\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# import nn\n",
    "import torch.nn as nn\n",
    "import rlace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3492e3d919142bf9e4bbd12760d38fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name=\"openai-community/gpt2-large\"\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,trust_remote_code=True,torch_dtype=torch.float16,\n",
    "                                             load_in_8bit=True,\n",
    "                                             device_map='auto').eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode(model, tokenizer, text, batch_size,layer=-1, pooling=\"last\"):\n",
    "  encodings = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "   for i in tqdm.tqdm(range(0, len(text), batch_size)):\n",
    "    batch = text[i:i+batch_size]\n",
    "    padded_tokens = tokenizer(batch, padding=True, return_tensors=\"pt\", max_length=128, truncation=True).to(\"cuda\")\n",
    "    outputs = model(**padded_tokens, output_hidden_states=True)\n",
    "    lengths = padded_tokens[\"attention_mask\"].sum(axis=1).detach().cpu().numpy()\n",
    "\n",
    "    hiddens = outputs.hidden_states[layer]\n",
    "    hiddens = hiddens.detach()\n",
    "    for h,l in zip(hiddens, lengths):\n",
    "      if pooling == \"last\":\n",
    "        h = h[l-1]\n",
    "      elif pooling == \"cls\":\n",
    "        h = h[0]\n",
    "      elif pooling == \"mean\":\n",
    "        h = h[:l].mean(axis=0)\n",
    "      encodings.append(h.detach().cpu().numpy())\n",
    "\n",
    "  return np.array(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bios_data/bios_data/bios_train.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    y = np.array([d[\"p\"] for d in data])\n",
    "    z = np.array([1 if d[\"g\"] == \"m\" else 0 for d in data])\n",
    "    texts = [d[\"text\"] for d in data]\n",
    "\n",
    "y_to_keep = [\"professor\", \"physician\", \"attorney\"]\n",
    "idx_to_keep = [i for i in range(len(y)) if y[i] in y_to_keep]\n",
    "y = y[idx_to_keep]\n",
    "z = z[idx_to_keep]\n",
    "texts = [texts[i] for i in idx_to_keep]\n",
    "\n",
    "num_m, num_f = sum(z), len(z) - sum(z)\n",
    "n = 10000#min(num_m, num_f)\n",
    "idx_m = [i for i in range(len(z)) if z[i] == 1]\n",
    "idx_f = [i for i in range(len(z)) if z[i] == 0]\n",
    "idx = idx_m[:n] + idx_f[:n]\n",
    "y = y[idx]\n",
    "z = z[idx]\n",
    "texts = [texts[i] for i in idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/625 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [12:54<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "layer = 16\n",
    "encodings = encode(model, tokenizer, texts, 32, layer=layer, pooling=\"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the texts, encodings, and labels\n",
    "\n",
    "with open(f\"bios_data/bios_data/encodings_{layer}.pickle\", \"wb\") as f:\n",
    "\n",
    "    pickle.dump({\"texts\": texts, \"encodings\": encodings, \"y\": y, \"z\": z}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000/15000. Acc post-projection: 50.150%; best so-far: 50.150%; Maj: 50.000%; Gap: 0.150%; best loss: 0.7001; current loss: 0.7001:  20%|##        | 3000/15000 [00:24<01:37, 122.80it/s] \n"
     ]
    }
   ],
   "source": [
    "# # import pca\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=64)\n",
    "# encodings_pca = pca.fit_transform(encodings)\n",
    "\n",
    "# output_rlace = rlace.solve_adv_game(encodings_pca, z, encodings_pca, z, rank=7, device=\"cpu\", out_iters=15000, in_iters_adv=1, in_iters_clf=1, epsilon=0.0015, batch_size=256, evalaute_every=1000, optimizer_params_P={\"lr\": 0.005, \"weight_decay\": 1e-4}, optimizer_params_predictor={\"lr\": 0.005, \"weight_decay\": 1e-4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P = np.eye(64) - output_rlace[\"P\"]\n",
    "# # perform truncated svd and collect eigenvalues. use numpy.linalg.eigh\n",
    "\n",
    "# output = np.linalg.eigh(P)\n",
    "# eigenvalues = list(sorted(output[0]))[::-1]\n",
    "# # plot eigenvalues\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.rcParams[\"font.family\"] = \"serif\"\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# plt.plot(eigenvalues[:16])\n",
    "# plt.xlabel(\"$K$\", fontsize=15)\n",
    "# plt.ylabel(\"Eigenvalue\", fontsize=15)\n",
    "# # make xlabel ticks ot be intetgers from 1 to 16\n",
    "# plt.xticks(np.arange(0, 16, 1))\n",
    "# plt.savefig(\"eigenvalues.pdf\", dpi=400)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def get_optimal_gaussian_transport_func(source_x, target_x):\n",
    "      cov_source = np.cov(source_x.T).real + 1e-8\n",
    "      cov_target = np.cov(target_x.T).real + 1e-8\n",
    "\n",
    "      # optimal transport\n",
    "\n",
    "      cov_source_sqrt = matrix_squared_root(cov_source)\n",
    "      cov_source_sqrt_inv = matrix_inv_squared_root(cov_source) #scipy.linalg.inv(cov_source_sqrt)\n",
    "\n",
    "      A = cov_source_sqrt_inv @ matrix_squared_root(cov_source_sqrt @ cov_target @ cov_source_sqrt) @ cov_source_sqrt_inv\n",
    "      return A\n",
    "\n",
    "def matrix_squared_root(A):\n",
    "    return scipy.linalg.sqrtm(A)\n",
    "\n",
    "\n",
    "def matrix_inv_squared_root(A):\n",
    "\n",
    "    return np.linalg.inv(matrix_squared_root(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_source = encodings[z==0,:][:]\n",
    "x_train_target = encodings[z==1,:][:]\n",
    "A = get_optimal_gaussian_transport_func(x_train_source, x_train_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, max_iter=10000, tol=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, max_iter=10000, tol=0.0001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(loss='log_loss', max_iter=10000, tol=0.0001)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_source = x_train_source.mean(axis=0)\n",
    "mean_target = x_train_target.mean(axis=0)\n",
    "encodings_transformed = encodings.copy()\n",
    "encodings_transformed[z == 0] = mean_target + (encodings_transformed[z == 0] - mean_source) @ A\n",
    "clf = SGDClassifier(loss=\"log_loss\", max_iter=10000, tol=1e-4)\n",
    "clf.fit(encodings, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterventionModule(nn.Module):\n",
    "    def __init__(self, mean_0, mean_1, A, mlp, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.mean_0 = torch.tensor(mean_0)\n",
    "        self.mean_1 = torch.tensor(mean_1)\n",
    "        self.A = torch.tensor(A)\n",
    "        self.mlp = sk2torch.wrap(mlp)\n",
    "        self.alpha = alpha\n",
    "        # set requires_grad=False to all params of the mlp\n",
    "        for p in self.mlp.parameters():\n",
    "            p.requires_grad = False\n",
    "    \n",
    "    def to_cuda(self, device):\n",
    "      self.A = self.A.to(device)\n",
    "      self.mean_0 = self.mean_0.to(device)\n",
    "      self.mean_1 = self.mean_1.to(device)\n",
    "      self.mlp = self.mlp.to(device)\n",
    "\n",
    "    def to_cpu(self):\n",
    "      self.A = self.A.to(\"cpu\")\n",
    "      self.mean_0 = self.mean_0.to(\"cpu\")\n",
    "      self.mean_1 = self.mean_1.to(\"cpu\")\n",
    "      self.mlp = self.mlp.to(\"cpu\")\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        self.to_cuda(hidden_states.device)\n",
    "        # fi hidden state is half, convert laso the params to half precision\n",
    "        if hidden_states.dtype == torch.float16:\n",
    "          self.A = self.A.half()\n",
    "          self.mean_0 = self.mean_0.half()\n",
    "          self.mean_1 = self.mean_1.half()\n",
    "          self.mlp = self.mlp.half()\n",
    "\n",
    "        \n",
    "        preds = self.mlp(hidden_states)\n",
    "        preds = preds[0]\n",
    "\n",
    "        x = hidden_states.clone().reshape(-1, hidden_states.shape[-1])\n",
    "        \n",
    "        if self.alpha != 0:\n",
    "            x[preds == 0] = self.alpha*self.mean_1 + (x[preds == 0] - self.alpha*self.mean_0)@self.A\n",
    "\n",
    "        #print(\"Steering {} samples\".format((preds == 0).sum()))\n",
    "        x = x.reshape(hidden_states.shape)\n",
    "        return x\n",
    "\n",
    "def insert_intervention(model, model_name, intervention, layer, after_layer_norm=False, replace_existing=False):\n",
    "    if \"gpt2\" in model_name.lower():\n",
    "        # if the mlp is already a Sequential object, do nothing\n",
    "        if isinstance(model.transformer.h[layer].mlp, torch.nn.Sequential):\n",
    "            return\n",
    "        if not replace_existing:\n",
    "            model.transformer.h[layer].mlp = torch.nn.Sequential(model.transformer.h[layer].mlp, intervention)\n",
    "        else:\n",
    "            \n",
    "            model.transformer.h[layer].mlp = torch.nn.Sequential(model.transformer.h[layer].mlp[:-1], intervention)\n",
    "\n",
    "    elif \"llama\" in model_name.lower():\n",
    "        # if the mlp is already a Sequential object, do nothing\n",
    "        if isinstance(model.model.layers[layer].post_attention_layernorm, torch.nn.Sequential):\n",
    "            return\n",
    "        if not replace_existing:\n",
    "            model.model.layers[layer].post_attention_layernorm = torch.nn.Sequential(model.model.layers[layer].post_attention_layernorm, intervention, model.model.layers[layer].post_attention_layernorm)\n",
    "        else:\n",
    "            model.model.layers[layer].post_attention_layernorm = torch.nn.Sequential(model.model.layers[layer].post_attention_layernorm[0], intervention, model.model.layers[layer].post_attention_layernorm[0])\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only GPT2 is supported\")\n",
    "\n",
    "\n",
    "def remove_intervention(model, model_name, layer):\n",
    "    if \"gpt2\" in model_name.lower():\n",
    "        # if the mlp is not a Sequential object, do nothing\n",
    "        if not isinstance(model.transformer.h[layer].mlp, torch.nn.Sequential):\n",
    "            return\n",
    "        model.transformer.h[layer].mlp = model.transformer.h[layer].mlp[0]\n",
    "\n",
    "    elif \"llama\" in model_name.lower():\n",
    "        # if the mlp is not a Sequential object, do nothing\n",
    "        if not isinstance(model.model.layers[layer].post_attention_layernorm, torch.nn.Sequential):\n",
    "            return\n",
    "        model.model.layers[layer].post_attention_layernorm = model.model.layers[layer].post_attention_layernorm[0]\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only GPT2 is supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_intervention(model, model_name, layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_module = InterventionModule(mean_source, mean_target, A, clf, alpha=0.0)\n",
    "remove_intervention(model, model_name, layer)\n",
    "insert_intervention(model, model_name, intervention_module, layer, replace_existing=False,after_layer_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Janet Kohn is a professor at NYU’s Steinhardt School of Culture, Education, and Human Development. She is a former Fulbright scholar and has been a visiting professor at the University of Paris. She has published numerous articles and books on the history of education and women’s education. She is the author of The Education of the Negro Prior to 1861 ('}]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "# generate text\n",
    "prompt = \"Janet Kohn is a professor at NYU\"\n",
    "generator(prompt, max_length=75, num_return_sequences=1, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Janet Kohn is a professor at NYU's Stern School of Business. She lives in New York and has four children.\"\n",
    "vocab_size = len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 10683/128256 [00:02<00:23, 5032.40it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/shauli/causal/causal.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.32.208.79/home/shauli/causal/causal.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msampling\u001b[39;00m \u001b[39mimport\u001b[39;00m counterfactual_generation\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B34.32.208.79/home/shauli/causal/causal.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m counterfactual_text \u001b[39m=\u001b[39m counterfactual_generation(model, tokenizer, \u001b[39m\"\u001b[39;49m\u001b[39mA new day has come\u001b[39;49m\u001b[39m\"\u001b[39;49m, vocab_size,prompt\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mA new\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/causal/sampling.py:143\u001b[0m, in \u001b[0;36mcounterfactual_generation\u001b[0;34m(model, tokenizer, sentence, vocab_size, prompt)\u001b[0m\n\u001b[1;32m    140\u001b[0m logit_j \u001b[39m=\u001b[39m logits[\u001b[39m0\u001b[39m][i][j]\n\u001b[1;32m    141\u001b[0m \u001b[39m#truncated_gumbel = TruncatedDistribution(gumbel_r, a=-300, b= value + logit_w - logit_j)\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39m#sample = truncated_gumbel.rvs(size=1)\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m sample \u001b[39m=\u001b[39m sample_from_truncated_gumbel(cdf_a, value \u001b[39m+\u001b[39;49m logit_w \u001b[39m-\u001b[39;49m logit_j, gumbel_r)\n\u001b[1;32m    144\u001b[0m gumbel_noise\u001b[39m.\u001b[39mappend(sample)\n\u001b[1;32m    145\u001b[0m \u001b[39m#gumbel_noise.append(1)\u001b[39;00m\n",
      "File \u001b[0;32m~/causal/sampling.py:113\u001b[0m, in \u001b[0;36msample_from_truncated_gumbel\u001b[0;34m(cdf_a, b, gumbel)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample_from_truncated_gumbel\u001b[39m(cdf_a,b,gumbel):\n\u001b[0;32m--> 113\u001b[0m     cdf_b \u001b[39m=\u001b[39m gumbel\u001b[39m.\u001b[39;49mcdf(b)\n\u001b[1;32m    114\u001b[0m     u \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m gumbel\u001b[39m.\u001b[39mppf(cdf_a \u001b[39m+\u001b[39m u \u001b[39m*\u001b[39m (cdf_b \u001b[39m-\u001b[39m cdf_a))\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/scipy/stats/_distn_infrastructure.py:2069\u001b[0m, in \u001b[0;36mrv_continuous.cdf\u001b[0;34m(self, x, *args, **kwds)\u001b[0m\n\u001b[1;32m   2067\u001b[0m cond0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_argcheck(\u001b[39m*\u001b[39margs) \u001b[39m&\u001b[39m (scale \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m   2068\u001b[0m cond1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_open_support_mask(x, \u001b[39m*\u001b[39margs) \u001b[39m&\u001b[39m (scale \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m-> 2069\u001b[0m cond2 \u001b[39m=\u001b[39m (x \u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m np\u001b[39m.\u001b[39;49masarray(_b)) \u001b[39m&\u001b[39m cond0\n\u001b[1;32m   2070\u001b[0m cond \u001b[39m=\u001b[39m cond0 \u001b[39m&\u001b[39m cond1\n\u001b[1;32m   2071\u001b[0m output \u001b[39m=\u001b[39m zeros(shape(cond), dtyp)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sampling import counterfactual_generation\n",
    "counterfactual_text = counterfactual_generation(model, tokenizer, \"A new day has come\", vocab_size,prompt=\"A new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A new component named as Bottle \\xa0']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counterfactual_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelProcessor(LogitsProcessor):\n",
    "    def __init__(self, precomputed_noise=None,noise=0, replaced_pairs=None):\n",
    "        self.precomputed_noise = precomputed_noise\n",
    "        self.i=0\n",
    "        self.replaced_pairs=replaced_pairs\n",
    "        # set np random seed\n",
    "\n",
    "        \n",
    "        #np.random.seed(noise)\n",
    "        self.noises = []\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        self.i += 1\n",
    "        if self.precomputed_noise is not None:\n",
    "            return scores + self.precomputed_noise[self.i - 1]\n",
    "        \n",
    "        gumbel = np.random.gumbel(loc=0.0, scale=1.0, size=scores.shape)\n",
    "        self.noises.append(gumbel)\n",
    "        return scores + gumbel\n",
    "\n",
    "\n",
    "def sample_from_truncated_gumbel(cdf_a,b,gumbel):\n",
    "    cdf_b = gumbel.cdf(b)\n",
    "    u = np.random.uniform(0, 1)\n",
    "    return gumbel.ppf(cdf_a + u * (cdf_b - cdf_a))\n",
    "\n",
    "def sample_gumbel(model, tokenizer, gumbel_processor, prompt):\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=64, logits_processor=[gumbel_processor],\n",
    "                                   do_sample=False, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id,)\n",
    "    return tokenizer.batch_decode(generate_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def sample_from_truncated_gumbel_vectorized(cdf_a, b_array, gumbel):\n",
    "    cdf_b = gumbel.cdf(b_array)  # Compute CDF for an array of b values\n",
    "    u = np.random.uniform(0, 1, size=b_array.shape)  # Generate uniform random values of the same shape as b_array\n",
    "    return gumbel.ppf(cdf_a + u * (cdf_b - cdf_a))  # Apply the inverse CDF to each element in the array\n",
    "\n",
    "def counterfactual_generation(model, tokenizer, sentence, vocab_size, prompt=None):\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
    "    logits = model(tokens).logits.detach().cpu().numpy()\n",
    "    all_gumbel_noise = []\n",
    "\n",
    "    cdf_a = gumbel_r.cdf(-500.0)\n",
    "\n",
    "    for i, w in enumerate(tokens[0][1:]):\n",
    "        # Generate Gumbel noise for each token in the vocabulary\n",
    "        gumbel_noises = np.random.gumbel(size=vocab_size)\n",
    "\n",
    "        # Calculate the logits differences\n",
    "        logit_diffs = logits[0][i] - logits[0][i][w]\n",
    "\n",
    "        # Apply vectorized truncated Gumbel sampling to the entire vocabulary\n",
    "        truncated_gumbels = sample_from_truncated_gumbel_vectorized(cdf_a, gumbel_noises + logit_diffs, gumbel_r)\n",
    "\n",
    "        # Ensure the original token keeps its specific noise\n",
    "        truncated_gumbels[w.detach().cpu().numpy().item()] = gumbel_noises[w.detach().cpu().numpy().item()]\n",
    "        \n",
    "        all_gumbel_noise.append(truncated_gumbels)\n",
    "\n",
    "    # Add a bias to the EOS token to make it more likely at the end\n",
    "    eos = tokenizer.eos_token_id\n",
    "    noise = np.zeros(vocab_size)\n",
    "    noise[eos] = 500.0\n",
    "    all_gumbel_noise.append(noise)\n",
    "\n",
    "    all_gumbel_noise = np.array(all_gumbel_noise)\n",
    "\n",
    "    # Convert Gumbel noise to a tensor\n",
    "    processor = GumbelProcessor(precomputed_noise=torch.tensor(all_gumbel_noise))\n",
    "\n",
    "    first_word = sentence.split(\" \")[0]\n",
    "    prompt = first_word if prompt is None else prompt\n",
    "    return sample_gumbel(model, tokenizer, processor, prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128256,)\n",
      "(128256,)\n",
      "(128256,)\n",
      "(128256,)\n",
      "(128256,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A new day has \\xa0 dawn for the']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counterfactual_generation(model, tokenizer, \"A new day has come\", vocab_size, prompt=\"A new day has\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shauli/miniconda3/envs/py39/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/shauli/miniconda3/envs/py39/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/shauli/miniconda3/envs/py39/lib/python3.9/site-packages/transformers/generation/utils.py:1797: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Janet Kohn is a professor at Kent State University. Her research focuses on information systems in higher education, modeling educational systems, and online communities. Dr. Kohn earned her doctorate degree in computer and information sciences at the University of Alabama.\\nRichard Andersen is an Associate Professor in the School of Computer Science and']\n",
      "['Janet Kohn is a professor at Kent State Law School. She is a graduate of DePaul College of Law. She also teaches at Capital Law School. She is a member of the bars of Ohio and Georgia. She co-authored \"Civil Procedure for the Paralegal\" with Daryl Waugh. The']\n"
     ]
    }
   ],
   "source": [
    "all_gumbel_noise = [np.random.gumbel(size=vocab_size) for _ in range(64)]\n",
    "processor = GumbelProcessor(precomputed_noise=torch.tensor(all_gumbel_noise))\n",
    "vocab_size=128256\n",
    "prompt = \"Janet Kohn is a professor at\"\n",
    "print(sample_gumbel(model, tokenizer, processor, prompt))\n",
    "processor = GumbelProcessor(precomputed_noise=torch.tensor(all_gumbel_noise))\n",
    "print(sample_gumbel(model, tokenizer, processor, prompt))\n",
    "#print(sample_gumbel(model, tokenizer, processor, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
